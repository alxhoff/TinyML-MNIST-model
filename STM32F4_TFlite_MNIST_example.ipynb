{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STM32F4 TFlite MNIST example",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgA+ALtZ2b2+q8uurpVXak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxhoff/TinyML-MNIST-model/blob/master/STM32F4_TFlite_MNIST_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_q-FH0dgMQq",
        "colab_type": "text"
      },
      "source": [
        "This example will perform character recognition through user input into the touch screen of an STM3240G-Evaluation board using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcNx6RbVg3cc",
        "colab_type": "code",
        "outputId": "eddbc52c-84f5-4420-b923-79d983139d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "!apt-get install -y xxd\n",
        "\n",
        "#! pip uninstall -y tensorflow\n",
        "#! pip install -q tf-nightly\n",
        "! pip install -q tensorflow-model-optimization\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 63%\r\rReading package lists... 63%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 68%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "xxd is already the newest version (2:8.0.1453-1ubuntu1.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y2hcinVg4Ws",
        "colab_type": "text"
      },
      "source": [
        "Importing Tensorflow allows you to use its API to load the MNIST dataset. It should be noted that we need to use TF version <1.14 as this version includes the fully connected operation version 3 which is incompatible with the micro interpreters version 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnCKuVlVjRhX",
        "colab_type": "code",
        "outputId": "66578c0d-c6f1-4893-f24c-b66ce9c2a969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "test_index = 12345\n",
        "print(labels_train[test_index])\n",
        "plt.imshow(images_train[test_index], cmap='Greys')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f73e3640e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-76bBO2rlABd",
        "colab_type": "text"
      },
      "source": [
        "The input of the neural network needs to know the input shape that it is going to be fed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWwrc29FlEgQ",
        "colab_type": "code",
        "outputId": "3510e2d9-a996-4515-98c2-a314a73afc51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_shape = images_train.shape\n",
        "print(\"{} images, each with shape of {} pixels x {} pixels\".format(input_shape[0], input_shape[1], input_shape[2]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 images, each with shape of 28 pixels x 28 pixels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn-3cvuymAWT",
        "colab_type": "text"
      },
      "source": [
        "The input shape for the model must be reshaped to 4D as the current shape does not show that each pixel is a 1D array where only the greyscale value (0-255) is stored. The input tensor's shape will be 3D as it will take a single-channel image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOZnWmzUmTuI",
        "colab_type": "code",
        "outputId": "230adf1a-4e1b-4d3f-ecbc-e57194c3a19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "images_train = images_train.reshape(images_train.shape[0], images_train.shape[1], images_train.shape[2], 1)\n",
        "images_test = images_test.reshape(images_test.shape[0], images_test.shape[1], images_test.shape[2], 1)\n",
        "input_tensor_shape = (images_test.shape[1], images_test.shape[2], 1)\n",
        "print(\"Input shape: {}\".format(input_shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape: (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjPMYztkm0tg",
        "colab_type": "text"
      },
      "source": [
        "The greyscale values stores in the images' pixels are 8 bit values and need to be normalized into floats between 0-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adD13BYinHlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_train = images_train.astype('float32')\n",
        "images_test = images_test.astype('float32')\n",
        "images_train /= 255\n",
        "images_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3oXnIHunXvs",
        "colab_type": "text"
      },
      "source": [
        "Now the convolutional NN that we will use to classify the input images taken from the touch screen will have the following layer structure\n",
        "\n",
        "1. Conv2D\n",
        "2. MaxPooling2D\n",
        "3. Flatten\n",
        "4. Dense\n",
        "5. Dropout\n",
        "11. Dense\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-2aWbOirRCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(28, kernel_size=(3,3), input_shape=input_tensor_shape))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model.add(keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "model2 = keras.Sequential()\n",
        "\n",
        "model2.add(keras.layers.Conv2D(12, kernel_size=(3,3), input_shape=input_tensor_shape))\n",
        "model2.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(keras.layers.Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model2.add(keras.layers.Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "model3 = keras.Sequential()\n",
        "\n",
        "model3.add(keras.layers.Conv2D(12, kernel_size=(3,3), strides=(2,2), input_shape=input_tensor_shape))\n",
        "model3.add(keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "model3.add(keras.layers.Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model3.add(keras.layers.Reshape((192,1)))\n",
        "model3.add(keras.layers.MaxPooling1D(pool_size=6, padding='valid', data_format='channels_last'))\n",
        "model3.add(keras.layers.Flatten())\n",
        "model3.add(keras.layers.Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "model4 = keras.Sequential()\n",
        "\n",
        "model4.add(keras.layers.Conv2D(12, kernel_size=(3,3), strides=(2,2), input_shape=input_tensor_shape))\n",
        "model4.add(keras.layers.Reshape( (13, 13, 12, 1)))\n",
        "model4.add(keras.layers.MaxPooling3D(pool_size=(6, 3,3)))\n",
        "model4.add(keras.layers.Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model4.add(keras.layers.Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "model5 = keras.Sequential()\n",
        "\n",
        "model5.add(keras.layers.Conv2D(12, kernel_size=(3,3), strides=(2,2), input_shape=input_tensor_shape))\n",
        "model5.add(keras.layers.Reshape( (338, 6, 1)))\n",
        "model5.add(keras.layers.MaxPooling2D(pool_size=(26, 3)))\n",
        "model5.add(keras.layers.Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model5.add(keras.layers.Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "quantized_model5 = quantize_model(model5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5CLXe63rRPf",
        "colab_type": "text"
      },
      "source": [
        "The Conv2D layer extracts features from the input image using filters that slide across the input image. In this case we will use 28 different filters to extract a large number of unique features that will then be used to classify each image in the later layers. Thus the output of this layer will have the shape (28, 28, 1, 28)\n",
        "\n",
        "MaxPooling2D is used to reduce the output size of the convolutional layer by reducing each 2 x 2 unique chunk of the output down in to a singular value, this reducing the output's size by a factor of 4. This will reduce our (28, 28, 1, 28) tensor down to a (7, 7, 1, 28) tensor.\n",
        "\n",
        "The Flatten layer then takes this 2D array (our image) and shapes it into a single dimension (1372).\n",
        "\n",
        "The following Dense layer reduces the input 1372 values down into 128 classes, taking the first steps in classifying the image into on of the 10 output classes (0-9). This is done using the relu activation function.\n",
        "\n",
        "The Dropout layer sets 20% of the tensor's values to 0 so as to reduce overfitting.\n",
        "\n",
        "Finally the last Dense layer reduces the output value down to the 10 classes, each representing a digit between 0 and 9. This is done using the softmax activation function which makes the outputs a set of probabilities summing to 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrG75fkHxdkN",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXWfgL0LxS3o",
        "colab_type": "code",
        "outputId": "a68c9d7d-51d2-4a81-d49f-4d519e9dee94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "quantized_model5.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "\n",
        "print(\"Model 1\")\n",
        "history = model.fit(x=images_train,y=labels_train, epochs=epochs, batch_size=batch_size)\n",
        "model.summary()\n",
        "\n",
        "print(\"Model 2\")\n",
        "history2 = model2.fit(x=images_train,y=labels_train, epochs=epochs, batch_size=batch_size)\n",
        "model2.summary()\n",
        "\n",
        "print(\"Model 3\")\n",
        "history3 = model3.fit(x=images_train,y=labels_train, epochs=epochs, batch_size=batch_size)\n",
        "model3.summary()\n",
        "\n",
        "print(\"Model 4\")\n",
        "history4 = model4.fit(x=images_train,y=labels_train, epochs=epochs, batch_size=batch_size)\n",
        "model4.summary()\n",
        "\n",
        "print(\"Model 5\")\n",
        "history5 = model5.fit(x=images_train,y=labels_train, epochs=epochs, batch_size=batch_size)\n",
        "model5.summary()\n",
        "\n",
        "print(\"Model 5 Quantized\")\n",
        "history5_q = quantized_model5.fit(x=images_train,y=labels_train, epochs=epochs, batch_size=batch_size)\n",
        "quantized_model5.summary()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 1\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2621 - accuracy: 0.9234\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1005 - accuracy: 0.9701\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0690 - accuracy: 0.9793\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0524 - accuracy: 0.9837\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0432 - accuracy: 0.9863\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0349 - accuracy: 0.9884\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0295 - accuracy: 0.9904\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0245 - accuracy: 0.9918\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0208 - accuracy: 0.9932\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0196 - accuracy: 0.9934\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 26, 26, 28)        280       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 28)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 4732)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               605824    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 607,394\n",
            "Trainable params: 607,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model 2\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4184 - accuracy: 0.8884\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.2166 - accuracy: 0.9378\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1666 - accuracy: 0.9536\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1375 - accuracy: 0.9608\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1190 - accuracy: 0.9668\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1048 - accuracy: 0.9699\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.0946 - accuracy: 0.9728\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.0855 - accuracy: 0.9757\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.0789 - accuracy: 0.9776\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.0724 - accuracy: 0.9793\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 26, 26, 12)        120       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 12)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 2028)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                20290     \n",
            "=================================================================\n",
            "Total params: 20,410\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model 3\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.5076 - accuracy: 0.5031\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.8099 - accuracy: 0.7430\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.6717 - accuracy: 0.7893\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5917 - accuracy: 0.8144\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5386 - accuracy: 0.8303\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4973 - accuracy: 0.8426\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4646 - accuracy: 0.8532\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4400 - accuracy: 0.8607\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4213 - accuracy: 0.8661\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4069 - accuracy: 0.8699\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 12)        120       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 12)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 192, 1)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 32, 1)             0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 450\n",
            "Trainable params: 450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model 4\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7612 - accuracy: 0.4223\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0361 - accuracy: 0.6767\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.8368 - accuracy: 0.7374\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.7174 - accuracy: 0.7768\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.6465 - accuracy: 0.7968\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.6024 - accuracy: 0.8086\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5739 - accuracy: 0.8161\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5531 - accuracy: 0.8220\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5380 - accuracy: 0.8260\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5261 - accuracy: 0.8300\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 13, 13, 12)        120       \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 13, 13, 12, 1)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 2, 4, 4, 1)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 450\n",
            "Trainable params: 450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model 5\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.7943 - accuracy: 0.4266\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 1.0960 - accuracy: 0.6661\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.9440 - accuracy: 0.7064\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.8728 - accuracy: 0.7288\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.8184 - accuracy: 0.7488\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.7671 - accuracy: 0.7672\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.7189 - accuracy: 0.7835\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.6767 - accuracy: 0.7972\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.6436 - accuracy: 0.8075\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.6176 - accuracy: 0.8150\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 13, 13, 12)        120       \n",
            "_________________________________________________________________\n",
            "reshape_5 (Reshape)          (None, 338, 6, 1)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 2, 1)          0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 26)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                270       \n",
            "=================================================================\n",
            "Total params: 390\n",
            "Trainable params: 390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model 5 Quantized\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 1.8120 - accuracy: 0.4198\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 1.1206 - accuracy: 0.6593\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.9752 - accuracy: 0.6950\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.9114 - accuracy: 0.7136\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.8638 - accuracy: 0.7300\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.8191 - accuracy: 0.7449\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.7793 - accuracy: 0.7596\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.7426 - accuracy: 0.7723\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.7104 - accuracy: 0.7831\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.6809 - accuracy: 0.7930\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_conv2d_9 (QuantizeWrap (None, 13, 13, 12)        147       \n",
            "_________________________________________________________________\n",
            "quant_reshape_5 (QuantizeWra (None, 338, 6, 1)         1         \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_7 (Quant (None, 13, 2, 1)          1         \n",
            "_________________________________________________________________\n",
            "quant_flatten_11 (QuantizeWr (None, 26)                1         \n",
            "_________________________________________________________________\n",
            "quant_dense_11 (QuantizeWrap (None, 10)                275       \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 390\n",
            "Non-trainable params: 35\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNtcEcKuxgYb",
        "colab_type": "text"
      },
      "source": [
        "Now we can evaluate our trained model using the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4zrA9hNxkAC",
        "colab_type": "code",
        "outputId": "27f79ff7-def7-4139-fd4b-66bc98f1c413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "res = model.evaluate(images_test, labels_test)\n",
        "print(\"Model1 has an accuracy of {0:.2f}%\".format(res[1] * 100))\n",
        "\n",
        "res = model2.evaluate(images_test, labels_test)\n",
        "print(\"Model2 has an accuracy of {0:.2f}%\".format(res[1] * 100))\n",
        "\n",
        "res = model3.evaluate(images_test, labels_test)\n",
        "print(\"Model3 has an accuracy of {0:.2f}%\".format(res[1] * 100))\n",
        "\n",
        "res = model4.evaluate(images_test, labels_test)\n",
        "print(\"Model4 has an accuracy of {0:.2f}%\".format(res[1] * 100))\n",
        "\n",
        "res = model5.evaluate(images_test, labels_test)\n",
        "print(\"Model5 has an accuracy of {0:.2f}%\".format(res[1] * 100))\n",
        "\n",
        "res = quantized_model5.evaluate(images_test, labels_test)\n",
        "print(\"Quantized model5 has an accuracy of {0:.2f}%\".format(res[1] * 100))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0496 - accuracy: 0.9860\n",
            "Model1 has an accuracy of 98.60%\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0822 - accuracy: 0.9745\n",
            "Model2 has an accuracy of 97.45%\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3852 - accuracy: 0.8783\n",
            "Model3 has an accuracy of 87.83%\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5035 - accuracy: 0.8375\n",
            "Model4 has an accuracy of 83.75%\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5959 - accuracy: 0.8236\n",
            "Model5 has an accuracy of 82.36%\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.8011\n",
            "Quantized model5 has an accuracy of 80.11%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeIhxRzT3w7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#best_model = quantized_model5\n",
        "best_model = model3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytu4OjmCzMIT",
        "colab_type": "text"
      },
      "source": [
        "Now we will need to convert and save our model into a downloadable file that will be appropriate for a microcontroller. This is done using the TensorFlow Lite Converter. The `TFLITE_BUILTINS` option tells the converter to only use TensorFlow Lite built in operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ8FDCOczYMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"MNIST_model_no_optimizations.tflite\", \"wb\").write(tflite_model)\n",
        "!xxd -i MNIST_model_no_optimizations.tflite > MNIST_model_no_optimizations.cc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6Eoe3x1B0U",
        "colab_type": "text"
      },
      "source": [
        "Now we will be running this model on a constrained device and as such we will need to perform quantization and optimizations to reduce the size of the model. The optimizations that TensorFlow offers are detailed [here](https://www.tensorflow.org/lite/performance/model_optimization). Also see [here](https://www.tensorflow.org/model_optimization/guide/quantization/training) for more information on quanitazation aware training and [here](https://www.tensorflow.org/lite/performance/post_training_quantization) for information on post-training quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9_WVtz1K5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"MNIST_optimized_for_size.tflite\", \"wb\").write(tflite_model)\n",
        "!xxd -i MNIST_optimized_for_size.tflite > MNIST_optimized_for_size.cc\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"MNIST_default_optimizations.tflite\", \"wb\").write(tflite_model)\n",
        "!xxd -i MNIST_default_optimizations.tflite > MNIST_default_optimizations.cc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ysFrDZq13Zx",
        "colab_type": "text"
      },
      "source": [
        "These optimizations reduced the file size significantly but we can do better. Enforcing full integer quantization for all operations as well as ensuring all model math is quantized to 8-bit. Doing so requires a representative dataset that can be used to evaluate optimizations made by the converter. The representative dataset must be a python iterator.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7WgyAFXCvk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def representative_dataset_gen():\n",
        "  for image in images_test:\n",
        "    array = np.array(image)\n",
        "    array = np.expand_dims(array, axis = 0)\n",
        "    yield ([array])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"MNIST_full_quanitization.tflite\", \"wb\").write(tflite_model)\n",
        "!xxd -i MNIST_full_quanitization.tflite > MNIST_full_quanitization.cc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JBLhgQ-2PXr",
        "colab_type": "text"
      },
      "source": [
        "This file needs to now be included in our microcontroller project and as such must be in a form that is easily usable on a device without a file system. xxd can be used to convert the model into a C source file where the model is stored as a byte array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Encu0Li72cmB",
        "colab_type": "code",
        "outputId": "41921de5-83a0-401f-a18e-779d62700e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!xxd -i MNIST_full_quanitization.tflite > MNIST_full_quanitization.cc\n",
        "!cat MNIST_full_quanitization.cc"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unsigned char MNIST_full_quanitization_tflite[] = {\n",
            "  0x08, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x4a, 0xfc, 0xff, 0xff,\n",
            "  0x03, 0x00, 0x00, 0x00, 0xac, 0x10, 0x00, 0x00, 0xa0, 0x03, 0x00, 0x00,\n",
            "  0x88, 0x03, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x12, 0x00, 0x00, 0x00,\n",
            "  0x78, 0x03, 0x00, 0x00, 0x64, 0x03, 0x00, 0x00, 0x28, 0x03, 0x00, 0x00,\n",
            "  0x04, 0x03, 0x00, 0x00, 0xe0, 0x02, 0x00, 0x00, 0xcc, 0x02, 0x00, 0x00,\n",
            "  0x78, 0x01, 0x00, 0x00, 0xf4, 0x00, 0x00, 0x00, 0xb0, 0x00, 0x00, 0x00,\n",
            "  0xa4, 0x00, 0x00, 0x00, 0x90, 0x00, 0x00, 0x00, 0x7c, 0x00, 0x00, 0x00,\n",
            "  0x68, 0x00, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00,\n",
            "  0x2c, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x04, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x14, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x24, 0xf1, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x34, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x44, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x54, 0xf1, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x64, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x74, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x84, 0xf1, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x12, 0xfc, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x30, 0x00, 0x00, 0x00, 0xb8, 0xdb, 0xff, 0xff, 0x9a, 0xd4, 0xff, 0xff,\n",
            "  0x19, 0xeb, 0xff, 0xff, 0x88, 0x02, 0x00, 0x00, 0x91, 0x01, 0x00, 0x00,\n",
            "  0xa5, 0xe0, 0xff, 0xff, 0x53, 0xcc, 0xff, 0xff, 0xcc, 0x15, 0xff, 0xff,\n",
            "  0x0c, 0xeb, 0xff, 0xff, 0xd4, 0x4e, 0xff, 0xff, 0x69, 0xc6, 0xff, 0xff,\n",
            "  0x10, 0x25, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x52, 0xfc, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x6c, 0x00, 0x00, 0x00, 0x8c, 0xb6, 0x2e, 0x38,\n",
            "  0x15, 0x3c, 0x81, 0x57, 0xbf, 0x3f, 0xaf, 0x81, 0x3d, 0x86, 0x6a, 0x6e,\n",
            "  0xc7, 0x88, 0xdc, 0xbe, 0xdd, 0x7f, 0x6b, 0xe0, 0x46, 0x02, 0xe6, 0x17,\n",
            "  0x2d, 0x7f, 0xf7, 0x05, 0x63, 0x14, 0x02, 0x54, 0x7f, 0x4c, 0x17, 0x26,\n",
            "  0x1b, 0x25, 0x2c, 0x23, 0x5c, 0xc7, 0xf6, 0xce, 0x9e, 0xdd, 0xea, 0x7f,\n",
            "  0xc3, 0xca, 0x2c, 0x33, 0x1d, 0x08, 0x41, 0x66, 0x85, 0xb0, 0x81, 0xd3,\n",
            "  0x4b, 0xb9, 0x88, 0xae, 0x7f, 0xac, 0xed, 0xb2, 0xaa, 0xaa, 0x81, 0x1b,\n",
            "  0x0d, 0xed, 0x54, 0x41, 0x53, 0x07, 0xd1, 0x81, 0xf8, 0x45, 0x2b, 0x00,\n",
            "  0x1e, 0x30, 0xff, 0x1b, 0x6a, 0xf6, 0xef, 0xe8, 0xd7, 0xe8, 0x81, 0xab,\n",
            "  0xa8, 0x82, 0x7f, 0x97, 0x43, 0xa1, 0xbc, 0x0b, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0xd2, 0xfc, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x01, 0x00, 0x00, 0xce, 0x1d, 0xe5, 0xd0, 0x0a, 0xec, 0x06, 0x02,\n",
            "  0xed, 0xed, 0x13, 0xef, 0x0c, 0xcd, 0x19, 0xfd, 0x2d, 0xba, 0x18, 0xd2,\n",
            "  0x97, 0xdb, 0x2b, 0xea, 0x07, 0xe2, 0x27, 0x1b, 0x05, 0x17, 0xf5, 0xde,\n",
            "  0xdd, 0xf1, 0x05, 0xe2, 0xf5, 0xd1, 0x24, 0xe7, 0xf0, 0xe5, 0xc9, 0xcd,\n",
            "  0x3d, 0xd6, 0xf1, 0xad, 0xd8, 0xb0, 0xdf, 0xd2, 0x45, 0xb1, 0xd2, 0xf5,\n",
            "  0x21, 0x0e, 0x06, 0xd7, 0x03, 0xd7, 0x25, 0x10, 0x03, 0x05, 0x05, 0x11,\n",
            "  0x0d, 0xff, 0xfe, 0xe7, 0xf5, 0x2f, 0xdf, 0x0b, 0xf9, 0xee, 0x06, 0xcf,\n",
            "  0xea, 0x05, 0xe2, 0x3b, 0x0e, 0x23, 0xf1, 0x0b, 0x25, 0x00, 0x2c, 0xf8,\n",
            "  0xfa, 0xe9, 0x02, 0x5b, 0xfe, 0x2b, 0xfb, 0x33, 0x09, 0x17, 0xf1, 0x07,\n",
            "  0xf9, 0x43, 0xd5, 0x3b, 0x15, 0x27, 0xfd, 0xd4, 0xd7, 0x15, 0xe3, 0x19,\n",
            "  0x0e, 0x0c, 0xf4, 0xfa, 0x1f, 0x1e, 0xec, 0x47, 0xf2, 0x2b, 0x0f, 0xb9,\n",
            "  0x0e, 0xf9, 0xf7, 0xd7, 0xf5, 0xcb, 0x1c, 0xff, 0xf5, 0xd0, 0x25, 0xcf,\n",
            "  0xe1, 0x8b, 0x07, 0xf2, 0x1e, 0xf8, 0x47, 0x15, 0x22, 0x01, 0x15, 0xf8,\n",
            "  0xe3, 0xc7, 0xda, 0xb3, 0x02, 0xb2, 0xde, 0x22, 0xfa, 0xef, 0xfa, 0xf8,\n",
            "  0xf1, 0x03, 0x0f, 0x3a, 0x00, 0xd3, 0x1f, 0xf6, 0xf4, 0x2f, 0xcd, 0x41,\n",
            "  0xfb, 0x22, 0x0b, 0xfe, 0xee, 0x23, 0xd7, 0x0e, 0xea, 0x36, 0xf2, 0x4d,\n",
            "  0xf9, 0x32, 0x14, 0xce, 0x0a, 0xf8, 0xf8, 0xab, 0x0c, 0xc9, 0x23, 0xfa,\n",
            "  0xf3, 0xc8, 0xff, 0xb3, 0xbc, 0xe9, 0xc6, 0x48, 0x2f, 0xc2, 0x25, 0xc7,\n",
            "  0xf8, 0x0d, 0x11, 0x10, 0xd5, 0xd3, 0x33, 0xed, 0x2b, 0x29, 0x0b, 0x06,\n",
            "  0x16, 0xf7, 0xdc, 0x22, 0xe8, 0x0f, 0xdc, 0x0d, 0x06, 0x26, 0x24, 0x3b,\n",
            "  0x27, 0x2a, 0x1c, 0x07, 0x08, 0xee, 0xcc, 0xf1, 0xee, 0xcb, 0x18, 0x20,\n",
            "  0xf3, 0x09, 0xeb, 0xba, 0x04, 0xb0, 0xca, 0xc8, 0xc7, 0x0a, 0xec, 0x0a,\n",
            "  0x12, 0x25, 0x0c, 0x12, 0x05, 0x1b, 0x0c, 0x0e, 0xe8, 0x1b, 0x05, 0x23,\n",
            "  0xdf, 0xf2, 0xfe, 0x04, 0x30, 0xff, 0xef, 0xfe, 0x08, 0xcf, 0x03, 0xe3,\n",
            "  0xd9, 0x28, 0xef, 0xfb, 0x81, 0xe4, 0xd1, 0xee, 0xd6, 0x4f, 0xd7, 0x0d,\n",
            "  0xfa, 0xe5, 0x12, 0x07, 0x05, 0x32, 0x06, 0xf9, 0x12, 0xb3, 0x04, 0x39,\n",
            "  0x0d, 0xfb, 0x21, 0x92, 0xdf, 0xe4, 0xd9, 0xee, 0xeb, 0xcd, 0xef, 0xfe,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x22, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x32, 0xfe, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x20, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x52, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xc0, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x72, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x28, 0x00, 0x00, 0x00,\n",
            "  0x3a, 0xfe, 0xff, 0xff, 0x72, 0x04, 0x00, 0x00, 0x01, 0x01, 0x00, 0x00,\n",
            "  0xa0, 0xfc, 0xff, 0xff, 0x09, 0x02, 0x00, 0x00, 0xf7, 0x01, 0x00, 0x00,\n",
            "  0xf5, 0xfe, 0xff, 0xff, 0xe0, 0x03, 0x00, 0x00, 0x2b, 0xfb, 0xff, 0xff,\n",
            "  0x65, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x24, 0xf4, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x34, 0xf4, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x4d, 0x4c, 0x49, 0x52,\n",
            "  0x20, 0x43, 0x6f, 0x6e, 0x76, 0x65, 0x72, 0x74, 0x65, 0x64, 0x2e, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00,\n",
            "  0x18, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00,\n",
            "  0x0e, 0x00, 0x00, 0x00, 0xd0, 0x02, 0x00, 0x00, 0xc4, 0x02, 0x00, 0x00,\n",
            "  0xb8, 0x02, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x6d, 0x61, 0x69, 0x6e, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x0b, 0x00, 0x00, 0x00, 0x7c, 0x02, 0x00, 0x00, 0x24, 0x02, 0x00, 0x00,\n",
            "  0xc0, 0x01, 0x00, 0x00, 0x88, 0x01, 0x00, 0x00, 0x54, 0x01, 0x00, 0x00,\n",
            "  0x00, 0x01, 0x00, 0x00, 0xbc, 0x00, 0x00, 0x00, 0x94, 0x00, 0x00, 0x00,\n",
            "  0x5c, 0x00, 0x00, 0x00, 0x28, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0xba, 0xfd, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x12, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xf6, 0xfe, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x06, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x76, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x80, 0x3f, 0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0f, 0x00, 0x00, 0x00, 0x26, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x08, 0x05, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x20, 0xf5, 0xff, 0xff,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0f, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x0e, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x3e, 0xfe, 0xff, 0xff, 0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x1a, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x1e, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x2c, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x04, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0xbe, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x05,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x4a, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x06, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00,\n",
            "  0x18, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x07, 0x00, 0x14, 0x00,\n",
            "  0x0e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x34, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x08, 0xf6, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x22, 0xff, 0xff, 0xff, 0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x0e, 0x00, 0x1a, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00,\n",
            "  0x07, 0x00, 0x14, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, 0x30, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x07, 0x00,\n",
            "  0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00, 0x0e, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x01, 0x03, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x09, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x0e, 0x00, 0x16, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00,\n",
            "  0x07, 0x00, 0x10, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x0a, 0x00, 0x10, 0x00, 0x07, 0x00, 0x08, 0x00, 0x0c, 0x00,\n",
            "  0x0a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x10, 0x00, 0x04, 0x00,\n",
            "  0x08, 0x00, 0x0c, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x11, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x12, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x11, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, 0x90, 0x09, 0x00, 0x00,\n",
            "  0xfc, 0x08, 0x00, 0x00, 0xa4, 0x08, 0x00, 0x00, 0x58, 0x08, 0x00, 0x00,\n",
            "  0x0c, 0x08, 0x00, 0x00, 0x78, 0x07, 0x00, 0x00, 0x0c, 0x06, 0x00, 0x00,\n",
            "  0x30, 0x05, 0x00, 0x00, 0xb4, 0x04, 0x00, 0x00, 0x20, 0x04, 0x00, 0x00,\n",
            "  0x94, 0x03, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x6c, 0x02, 0x00, 0x00,\n",
            "  0xd8, 0x01, 0x00, 0x00, 0x54, 0x01, 0x00, 0x00, 0xd0, 0x00, 0x00, 0x00,\n",
            "  0x74, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0xd8, 0xff, 0xff, 0xff, 0x18, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x49, 0x64, 0x65, 0x6e, 0x74, 0x69, 0x74, 0x79,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x0a, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x0c, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x63, 0x6f, 0x6e, 0x76,\n",
            "  0x32, 0x64, 0x5f, 0x37, 0x5f, 0x69, 0x6e, 0x70, 0x75, 0x74, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00,\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x32, 0xf7, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x44, 0x00, 0x00, 0x00, 0x11, 0x00, 0x00, 0x00,\n",
            "  0x28, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xb4, 0xf7, 0xff, 0xff,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x80, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x80, 0x3b, 0x0d, 0x00, 0x00, 0x00, 0x49, 0x64, 0x65, 0x6e,\n",
            "  0x74, 0x69, 0x74, 0x79, 0x5f, 0x69, 0x6e, 0x74, 0x38, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x8a, 0xf7, 0xff, 0xff, 0x00, 0x00, 0x00, 0x09, 0x6c, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x7c, 0xf7, 0xff, 0xff, 0x2c, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x12, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x7f, 0x55, 0x19, 0x3e, 0x01, 0x00, 0x00, 0x00, 0x87, 0x3c, 0x82, 0x41,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xce, 0x3b, 0xaf, 0xc1, 0x1c, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37,\n",
            "  0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x39, 0x2f, 0x42, 0x69, 0x61,\n",
            "  0x73, 0x41, 0x64, 0x64, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x0a, 0xf8, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x6c, 0x00, 0x00, 0x00, 0x0f, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xfc, 0xf7, 0xff, 0xff,\n",
            "  0x2c, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xac, 0xff, 0xff, 0xff,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x2c, 0xe2, 0x16, 0x3d,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x39, 0x51, 0xf8, 0x40, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x68, 0x15, 0xd1, 0xbf, 0x1e, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x66, 0x6c, 0x61,\n",
            "  0x74, 0x74, 0x65, 0x6e, 0x5f, 0x39, 0x2f, 0x52, 0x65, 0x73, 0x68, 0x61,\n",
            "  0x70, 0x65, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x20, 0x00, 0x00, 0x00, 0x8a, 0xf8, 0xff, 0xff, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x78, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x44, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x7c, 0xf8, 0xff, 0xff, 0x30, 0x00, 0x00, 0x00,\n",
            "  0x24, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xac, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x2c, 0xe2, 0x16, 0x3d,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x39, 0x51, 0xf8, 0x40, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x68, 0x15, 0xd1, 0xbf, 0x24, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x6d, 0x61, 0x78,\n",
            "  0x5f, 0x70, 0x6f, 0x6f, 0x6c, 0x69, 0x6e, 0x67, 0x31, 0x64, 0x5f, 0x31,\n",
            "  0x2f, 0x53, 0x71, 0x75, 0x65, 0x65, 0x7a, 0x65, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x1a, 0xf9, 0xff, 0xff, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x74, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x0c, 0xf9, 0xff, 0xff, 0x2c, 0x00, 0x00, 0x00,\n",
            "  0x20, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xac, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x2c, 0xe2, 0x16, 0x3d, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x39, 0x51, 0xf8, 0x40, 0x01, 0x00, 0x00, 0x00, 0x68, 0x15, 0xd1, 0xbf,\n",
            "  0x24, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x6d, 0x61, 0x78, 0x5f, 0x70, 0x6f, 0x6f,\n",
            "  0x6c, 0x69, 0x6e, 0x67, 0x31, 0x64, 0x5f, 0x31, 0x2f, 0x4d, 0x61, 0x78,\n",
            "  0x50, 0x6f, 0x6f, 0x6c, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xaa, 0xf9, 0xff, 0xff, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x74, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x9c, 0xf9, 0xff, 0xff, 0x2c, 0x00, 0x00, 0x00,\n",
            "  0x20, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xac, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x2c, 0xe2, 0x16, 0x3d, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x39, 0x51, 0xf8, 0x40, 0x01, 0x00, 0x00, 0x00, 0x68, 0x15, 0xd1, 0xbf,\n",
            "  0x27, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x6d, 0x61, 0x78, 0x5f, 0x70, 0x6f, 0x6f,\n",
            "  0x6c, 0x69, 0x6e, 0x67, 0x31, 0x64, 0x5f, 0x31, 0x2f, 0x45, 0x78, 0x70,\n",
            "  0x61, 0x6e, 0x64, 0x44, 0x69, 0x6d, 0x73, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xc0, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x3a, 0xfa, 0xff, 0xff, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x70, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00, 0x44, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x2c, 0xfa, 0xff, 0xff, 0x30, 0x00, 0x00, 0x00,\n",
            "  0x24, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xec, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x50, 0x01, 0x58, 0x3d,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x39, 0x51, 0xf8, 0x40, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x65, 0x01, 0xb6, 0xc0, 0x1e, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x72, 0x65, 0x73,\n",
            "  0x68, 0x61, 0x70, 0x65, 0x5f, 0x33, 0x2f, 0x52, 0x65, 0x73, 0x68, 0x61,\n",
            "  0x70, 0x65, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0xc0, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xc2, 0xfa, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x74, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xb4, 0xfa, 0xff, 0xff,\n",
            "  0x2c, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xec, 0xff, 0xff, 0xff,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x50, 0x01, 0x58, 0x3d,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x39, 0x51, 0xf8, 0x40, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x65, 0x01, 0xb6, 0xc0, 0x24, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x6d, 0x61, 0x78,\n",
            "  0x5f, 0x70, 0x6f, 0x6f, 0x6c, 0x69, 0x6e, 0x67, 0x32, 0x64, 0x5f, 0x36,\n",
            "  0x2f, 0x4d, 0x61, 0x78, 0x50, 0x6f, 0x6f, 0x6c, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x52, 0xfb, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x5c, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00,\n",
            "  0x44, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x44, 0xfb, 0xff, 0xff,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xec, 0xff, 0xff, 0xff,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x50, 0x01, 0x58, 0x3d, 0x01, 0x00, 0x00, 0x00, 0x39, 0x51, 0xf8, 0x40,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x65, 0x01, 0xb6, 0xc0, 0x0b, 0x00, 0x00, 0x00,\n",
            "  0x74, 0x66, 0x6c, 0x2e, 0x63, 0x6f, 0x6e, 0x76, 0x5f, 0x32, 0x64, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00,\n",
            "  0x0d, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0xca, 0xfb, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x02, 0xc8, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00,\n",
            "  0xac, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x4c, 0xfc, 0xff, 0xff,\n",
            "  0x6c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x01, 0xab, 0xfa, 0x36, 0x49, 0x8c, 0xca, 0x36,\n",
            "  0x6b, 0x84, 0x45, 0x37, 0x80, 0x06, 0x25, 0x38, 0x4b, 0x0d, 0x7f, 0x38,\n",
            "  0x2d, 0x99, 0x09, 0x37, 0x58, 0xc3, 0x18, 0x38, 0x58, 0x73, 0xc2, 0x36,\n",
            "  0xe5, 0xde, 0x89, 0x38, 0x1a, 0xed, 0x03, 0x37, 0x98, 0x99, 0xc8, 0x37,\n",
            "  0xfc, 0x3e, 0xd2, 0x36, 0x0c, 0x00, 0x00, 0x00, 0x73, 0x74, 0x64, 0x2e,\n",
            "  0x63, 0x6f, 0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0xa2, 0xfc, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x4c, 0x01, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00,\n",
            "  0x20, 0x01, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x94, 0xfc, 0xff, 0xff,\n",
            "  0xe0, 0x00, 0x00, 0x00, 0xa8, 0x00, 0x00, 0x00, 0x70, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x55, 0xb0, 0xf9, 0x3a, 0xbc, 0xc1, 0xc9, 0x3a,\n",
            "  0xe6, 0xbe, 0x44, 0x3b, 0x79, 0x61, 0x24, 0x3c, 0x3d, 0x0e, 0x7e, 0x3c,\n",
            "  0x93, 0x0f, 0x09, 0x3b, 0x94, 0x2a, 0x18, 0x3c, 0xe4, 0xb0, 0xc1, 0x3a,\n",
            "  0x06, 0x55, 0x89, 0x3c, 0x2c, 0x69, 0x03, 0x3b, 0xfe, 0xd0, 0xc7, 0x3b,\n",
            "  0xbc, 0x6c, 0xd1, 0x3a, 0x0c, 0x00, 0x00, 0x00, 0x51, 0xfc, 0x28, 0x3e,\n",
            "  0xab, 0x68, 0x2d, 0x3e, 0x68, 0x35, 0xc3, 0x3e, 0xb6, 0x18, 0xa3, 0x3f,\n",
            "  0x21, 0x12, 0xfc, 0x3f, 0x74, 0xfd, 0x87, 0x3e, 0x88, 0x5c, 0x71, 0x3f,\n",
            "  0x82, 0x2d, 0x40, 0x3e, 0x65, 0xf7, 0xb3, 0x3f, 0x70, 0x12, 0x0d, 0x3e,\n",
            "  0xc9, 0x39, 0x25, 0x3f, 0xe3, 0xc9, 0x4f, 0x3e, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0xf4, 0xbc, 0x77, 0xbe, 0x39, 0x2e, 0x48, 0xbe, 0x71, 0xf4, 0x49, 0xbe,\n",
            "  0x66, 0x3c, 0xaf, 0xbd, 0xa7, 0xa5, 0xb3, 0x3e, 0x75, 0xa8, 0x52, 0xbe,\n",
            "  0x3f, 0xfa, 0x96, 0xbf, 0x30, 0x4b, 0x35, 0xbe, 0x5c, 0x42, 0x08, 0xc0,\n",
            "  0x5a, 0x62, 0x82, 0xbe, 0x5c, 0x41, 0x46, 0xbf, 0x33, 0xf1, 0x4e, 0xbe,\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x63, 0x6f, 0x6e, 0x76, 0x32, 0x64, 0x5f,\n",
            "  0x37, 0x2f, 0x43, 0x6f, 0x6e, 0x76, 0x32, 0x44, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x0a, 0xfe, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x6c, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x44, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xfc, 0xfd, 0xff, 0xff,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x44, 0x0b, 0x48, 0x3c, 0x01, 0x00, 0x00, 0x00, 0x97, 0xd3, 0x8d, 0x3f,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x2d, 0x7b, 0xc6, 0xbf, 0x1b, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37,\n",
            "  0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x39, 0x2f, 0x4d, 0x61, 0x74,\n",
            "  0x4d, 0x75, 0x6c, 0x00, 0x02, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x20, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x07, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x0e, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x02, 0x05, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x6c, 0xff, 0xff, 0xff, 0x2b, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37,\n",
            "  0x2f, 0x6d, 0x61, 0x78, 0x5f, 0x70, 0x6f, 0x6f, 0x6c, 0x69, 0x6e, 0x67,\n",
            "  0x31, 0x64, 0x5f, 0x31, 0x2f, 0x45, 0x78, 0x70, 0x61, 0x6e, 0x64, 0x44,\n",
            "  0x69, 0x6d, 0x73, 0x2f, 0x64, 0x69, 0x6d, 0x00, 0xe2, 0xfe, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x02, 0x38, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xb8, 0xff, 0xff, 0xff,\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x66, 0x6c, 0x61, 0x74, 0x74, 0x65, 0x6e,\n",
            "  0x5f, 0x39, 0x2f, 0x43, 0x6f, 0x6e, 0x73, 0x74, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x2a, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x02, 0x44, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x04, 0x00, 0x04, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37, 0x2f, 0x72, 0x65, 0x73,\n",
            "  0x68, 0x61, 0x70, 0x65, 0x5f, 0x33, 0x2f, 0x52, 0x65, 0x73, 0x68, 0x61,\n",
            "  0x70, 0x65, 0x2f, 0x73, 0x68, 0x61, 0x70, 0x65, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x7e, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x02, 0x70, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x38, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x0c, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xac, 0xce, 0xeb, 0x39, 0x2b, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x37,\n",
            "  0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x39, 0x2f, 0x42, 0x69, 0x61,\n",
            "  0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61, 0x64, 0x56, 0x61, 0x72,\n",
            "  0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x0a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x08, 0x00,\n",
            "  0x07, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00, 0x0e, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x6c, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x4c, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x14, 0x00,\n",
            "  0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x2c, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x80, 0xff, 0xff, 0xff,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x81, 0x80, 0x80, 0x3b,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x80, 0x3f, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, 0x63, 0x6f, 0x6e, 0x76,\n",
            "  0x32, 0x64, 0x5f, 0x37, 0x5f, 0x69, 0x6e, 0x70, 0x75, 0x74, 0x5f, 0x69,\n",
            "  0x6e, 0x74, 0x38, 0x00, 0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x09, 0x00, 0x00, 0x00, 0x94, 0x00, 0x00, 0x00, 0x78, 0x00, 0x00, 0x00,\n",
            "  0x64, 0x00, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x48, 0x00, 0x00, 0x00,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x9a, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x06,\n",
            "  0x02, 0x00, 0x00, 0x00, 0xde, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x72,\n",
            "  0xae, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x19, 0x02, 0x00, 0x00, 0x00,\n",
            "  0xd2, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x09, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x07, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x2b, 0xfa, 0xff, 0xff, 0xff, 0x00, 0x46, 0x06, 0x00,\n",
            "  0x06, 0x00, 0x05, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x16, 0x0a, 0x00,\n",
            "  0x0e, 0x00, 0x07, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x11, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0a, 0x00,\n",
            "  0x0c, 0x00, 0x07, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x03, 0x03, 0x00, 0x00, 0x00\n",
            "};\n",
            "unsigned int MNIST_full_quanitization_tflite_len = 4448;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvBIhOTI2u1J",
        "colab_type": "text"
      },
      "source": [
        "This file is downloadable from the left hand menu of colab. Although this file is too large to be used as a model on a microcontroller. We will have to do some more optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw7zP9zP80xP",
        "colab_type": "text"
      },
      "source": [
        "Pruning is where unncessart values in weight tensors are removed, thus reducing the number of parameters needed to store the model. This is done by further training the model while intermittently running pruning code that removes unnecessary connections between the layers of the neuran network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rah2i039XJg",
        "colab_type": "code",
        "outputId": "75449208-8f14-4d9d-e28f-e056249a8371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "!pip install -q tensorflow-model-optimization\n",
        "\n",
        "import numpy as np \n",
        "import os\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "end_step = np.ceil(1.0 * images_train.shape[0] / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "pruning_params = {\n",
        "    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.5,\n",
        "                                                 final_sparsity=0.9,\n",
        "                                                 begin_step=0,\n",
        "                                                 end_step=end_step,\n",
        "                                                 frequency=100\n",
        "                                                 )\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(best_model, **pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [sparsity.UpdatePruningStep(),sparsity.PruningSummaries(log_dir=os.path.abspath(os.getcwd()), profile_batch=0)]\n",
        "\n",
        "new_pruned_model.fit(images_train, labels_train,\n",
        "                     batch_size=batch_size,\n",
        "                     epochs=epochs,\n",
        "                     verbose=1,\n",
        "                     callbacks=callbacks)\n",
        "\n",
        "score = new_pruned_model.evaluate(images_test, labels_test, verbose=0)\n",
        "print(\"Test loss: {}\".format(score[0]))\n",
        "print(\"Test accuracy: {}\".format(score[1]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d_7 (None, 13, 13, 12)        230       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 4, 4, 12)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 192)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_reshape_ (None, 192, 1)            1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 32, 1)             1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 32)                1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_9  (None, 10)                652       \n",
            "=================================================================\n",
            "Total params: 887\n",
            "Trainable params: 450\n",
            "Non-trainable params: 437\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.5450 - accuracy: 0.8155\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 0.5529 - accuracy: 0.8146\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 0.6557 - accuracy: 0.7856\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.7240 - accuracy: 0.7591\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.8128 - accuracy: 0.7283\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 0.9207 - accuracy: 0.6831\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1.0823 - accuracy: 0.6261\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1.0271 - accuracy: 0.6425\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 0.9970 - accuracy: 0.6531\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.9879 - accuracy: 0.6561\n",
            "Test loss: 0.9642811417579651\n",
            "Test accuracy: 0.6690000295639038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ctnEnjAzKu",
        "colab_type": "text"
      },
      "source": [
        "Now we can export the pruned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6oh6ZXXA1BF",
        "colab_type": "code",
        "outputId": "670d967f-1551-49c8-8029-c1f8794e472a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "new_pruned_model.summary()\n",
        "best_model_pruned = sparsity.strip_pruning(new_pruned_model)\n",
        "best_model_pruned.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d_7 (None, 13, 13, 12)        230       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 4, 4, 12)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 192)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_reshape_ (None, 192, 1)            1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 32, 1)             1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 32)                1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_9  (None, 10)                652       \n",
            "=================================================================\n",
            "Total params: 887\n",
            "Trainable params: 450\n",
            "Non-trainable params: 437\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 12)        120       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 12)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 192, 1)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 32, 1)             0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 450\n",
            "Trainable params: 450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSzcstilCDGL",
        "colab_type": "text"
      },
      "source": [
        "We can compare each pruned weight against zero to see how many weights were pruned out of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKhsXQz6CKCG",
        "colab_type": "code",
        "outputId": "7fd044b1-b0ef-46fa-f2df-2bb864ba21a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for i, w in enumerate(best_model_pruned.get_weights()):\n",
        "    print(\"{} -- Total:{}, Zeros: {:.2f}%\".format(\n",
        "        model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100 ))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_5/kernel:0 -- Total:108, Zeros: 89.81%\n",
            "conv2d_5/bias:0 -- Total:12, Zeros: 0.00%\n",
            "dense_6/kernel:0 -- Total:320, Zeros: 90.00%\n",
            "dense_6/bias:0 -- Total:10, Zeros: 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtYoMRRnChQh",
        "colab_type": "text"
      },
      "source": [
        "Same as before we can quantize the model, convert and save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkFxO3jwCrF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model_pruned)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"MNIST_pruned.tflite\", \"wb\").write(tflite_model)\n",
        "!xxd -i MNIST_pruned.tflite > MNIST_pruned.cc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKmWVr-UDCLM",
        "colab_type": "text"
      },
      "source": [
        "Puning saved us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HWDND29DD3s",
        "colab_type": "code",
        "outputId": "63c19bdd-74ed-4dc5-ee55-ee1668968a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Quanized file size difference is: {} bytes\".format(os.path.getsize(\"MNIST_model_no_optimizations.tflite\") - os.path.getsize(\"MNIST_full_quanitization.tflite\")))\n",
        "print(\"Pruned file size difference is: {} bytes\".format(os.path.getsize(\"MNIST_full_quanitization.tflite\") - os.path.getsize(\"MNIST_pruned.tflite\")))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quanized file size difference is: -236 bytes\n",
            "Pruned file size difference is: 0 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGkaLQ3rfvI",
        "colab_type": "text"
      },
      "source": [
        "As the generated models are so large we need to make sure they are stored in the .rodata segment. To do so the model must be prefixed extern const such that it does not land in the .data segment and kill our ram. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fsVhonWuvV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sed -i '1s/^/extern const /' MNIST_full_quanitization.cc\n",
        "!sed -i '$s/^/extern const /' MNIST_full_quanitization.cc"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}